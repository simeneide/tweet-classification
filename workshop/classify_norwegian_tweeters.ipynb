{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Twitter users based on their tweets\n",
    "In this workshop we will figure out who wrote which tweets from some Norwegian famous tweeters.\n",
    "\n",
    "#### Structure of the notebook\n",
    "There are three sections in this notebook: \n",
    "- Build Dataset, \n",
    "- Build machine learning model, and \n",
    "- See how well the model works.\n",
    "\n",
    "The first section is not really needed to know well.\n",
    "It contains all the \"boring\" data preprocessing that is necessary to prepare the data for the algorithm.\n",
    "For completeness it is here and commented, so that you can go through it if you really want to.\n",
    "\n",
    "The second part is where we will focus on during this workshop. \n",
    "For now, skim through the preprocessing, but really start at the second section.\n",
    "The third section is an evaluation of how good your model is.\n",
    "At the end there is some tips to get you going to create an even better model than this!\n",
    "\n",
    "#### Jupyter notebooks\n",
    "This jupyter notebook creates a neural network that use the tweet to predict the author. \n",
    "A jupyter notebook is a file where you can run code and show results in the same document. \n",
    "If you want a introduction, you can look [here](https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/), \n",
    "but the main idea is that you have markdown chunks (like this one) and code chunks. \n",
    "The notebook has a 'python console' running in the background.\n",
    "To run a chunk press **shift+enter**. Then the code is executed and you can see the results. To add a chunk press **esc+a** and an empty chunk will be create above where you stand :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import all needed libraries\n",
    "import datetime\n",
    "import jsonlines\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset\n",
    "First we need to build our dataset.\n",
    "This is not really part of the workshop, but the preparation code is left here so that those interested can check it out later.\n",
    "In the data directory there is some files from the twitter users we will investigate.\n",
    "These files contains all the tweets the user have published, and is pre-downloaded for the workshop.\n",
    "Basically what we do is the following:\n",
    "\n",
    "- Read the files and put them in a nice pandas dataframe\n",
    "- Build a vocabulary of all words and give them indicies\n",
    "- Transform all tweets into indicies (something required by tensorflow later)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files:\n",
    "def load_files_and_create_dataframe():\n",
    "    files = ['data/' + f for f in os.listdir(\"data\")]\n",
    "    L = []\n",
    "    for path in files:\n",
    "        try:\n",
    "            with jsonlines.open(path, mode='r') as reader:\n",
    "                tmp = [line for line in reader.iter()]\n",
    "                if len(tmp) > 50 & isinstance(tmp, list):\n",
    "                    L.extend(tmp)\n",
    "                else:\n",
    "                    print(\"%s had less than 50 tweets. skipping..\" %path)\n",
    "        except:\n",
    "            print(\"Did not manage to process: %s\" % path) \n",
    "            \n",
    "    raw = pd.DataFrame(L)\n",
    "    raw.timestamp = pd.to_datetime(raw.timestamp)\n",
    "    \n",
    "    # Shuffle dataset and filter out some users that have sneaked into the files \n",
    "    # (i.e. have less than 50 observations)\n",
    "    \n",
    "    counts = raw.user.value_counts()\n",
    "    counts = counts[counts > 50]\n",
    "    keep_users = counts.index\n",
    "    \n",
    "    raw = (\n",
    "        raw[raw.user.isin(keep_users)]\n",
    "        .sample(frac=1) # shuffle dataset\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return raw\n",
    "\n",
    "raw = load_files_and_create_dataframe()\n",
    "\n",
    "print(\"We got %d observations/tweets in our dataset. The first 5 looks like this:\" % raw.shape[0])\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build vocabulary\n",
    "We build a vocabulary of the \"known\" words in the dataset.\n",
    "Essentially we will require that we have seen the word 5 times before for it to be in the vocabulary.\n",
    "Otherwise, we will replace the word with \"UNK\".\n",
    "\n",
    "The rest is pretty straightforward. \n",
    "We split by space and only keep the most important characters in the words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(u\"([.!?])\", r\" \", s)\n",
    "    \n",
    "    # Let @ and # be independent words:\n",
    "    s = re.sub(u\"(#)\", r\"# \", s)\n",
    "    s = re.sub(u\"(@)\", r\"@ \", s)\n",
    "    \n",
    "    # Remove all other characters than the alphabet\n",
    "    s = re.sub(u\"[^a-zA-Z.!#@?\\xf8\\xe6\\xe5]+\", u\" \", s)\n",
    "    s = s.split(\" \")\n",
    "    return s\n",
    "\n",
    "def build_vocabulary(text, min_count = 5):\n",
    "    normalized = text.map(tokenizeString)\n",
    "    all_words = np.array([item for sublist in normalized.values.tolist() for item in sublist])\n",
    "    words, counts = np.unique(all_words, return_counts=True)\n",
    "    keep_words = words[counts > min_count]\n",
    "    ind2word = {i + 2 : w for i, w in enumerate(keep_words)}\n",
    "    ind2word[1] = \"UNK\"\n",
    "    ind2word[0] = \"EMPTY\"\n",
    "    word2ind = {w : i for i, w in ind2word.items()}\n",
    "    \n",
    "    return word2ind, ind2word\n",
    "\n",
    "word2ind, ind2word = build_vocabulary(raw.text)\n",
    "print(\"We have a total vocabulary of %d words.\" % len(word2ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"If you want to find the index of a word:\")\n",
    "print( word2ind[\"hus\"] )\n",
    "print(\"Similarly: If you want to find the word corresponding to an index:\")\n",
    "print( ind2word[1000] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess text\n",
    "Now we want to transform all tweets from words to a fixed length index.\n",
    "We do this in two steps:\n",
    "- First we transform each word into an index (the vocabulary we defined above)\n",
    "- Then we set all tweets to a length of 40. If a tweet does not have 40 words then we fill with empty words. If the tweet have more, we truncate the end.\n",
    "\n",
    "The whole process is contained in the encode_text function. Also see that we have a decode_tweet function to go back again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequences:\n",
    "vectorize_tweet = lambda x: [word2ind.get(w,1) for w in x]\n",
    "PAD_LENGTH = 40\n",
    "\n",
    "def encode_text(s, padlen = PAD_LENGTH):\n",
    "    tokenized = tokenizeString(s)\n",
    "    vectorized = vectorize_tweet(tokenized)\n",
    "    padded = pad_sequences([vectorized], maxlen = padlen, padding = \"post\", truncating= \"post\")\n",
    "    return padded\n",
    "\n",
    "def decode_tweet(vec):\n",
    "    dec = [ind2word.get(ind) for ind in vec if ind != 0]\n",
    "    text = \" \".join(dec)\n",
    "    \n",
    "    # Assume all @ and # is followed by word without space:\n",
    "    text = re.sub(u\"# \", r\"#\", text)\n",
    "    text = re.sub(u\"@ \", r\"@\", text)\n",
    "    return text\n",
    "\n",
    "# All the work happens here:\n",
    "X = np.squeeze(np.array(raw.text.map(encode_text).values.tolist()))#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We now have all our tweets in the numpy array X. Each row is a tweet (in total %d), and each column is a word (in total %d)\" % (X.shape[0], X.shape[1]))\n",
    "\n",
    "print(\"We can easily decode a tweet by doing the following:\")\n",
    "print('--')\n",
    "decode_tweet(X[300,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess labels\n",
    "Each author represent a class. The classes needs to be represented by indicies.\n",
    "Therefore, we will also transform all the usernames into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classes(labels):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(labels)\n",
    "    label_index = le.transform(labels)\n",
    "    ind2class = {i : user for i, user in  enumerate(le.classes_)}\n",
    "    return label_index, ind2class\n",
    "\n",
    "raw['user_class'], ind2class = build_classes(raw['user'])\n",
    "N_classes = len(ind2class) # Store total number of authors somewhere\n",
    "print(\"The index -> author mapping can be found in this dictionary:\")\n",
    "ind2class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = raw.user_class.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now y contains all the indicies of each user:\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can find the author of the tweet above:\n",
    "y[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can transform the index back to the user by using the ind2class dictionary:\n",
    "ind2class[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further, it could be interesting to know how many tweets are from each author. This is a summary:\n",
    "raw.groupby(\"user\")['user'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into training and test\n",
    "When we build our model it is important that we test on a different dataset than we train on.\n",
    "Otherwise, we could just have written a lookup table to predict each tweet author.\n",
    "Instead, we want the model to be able to generalize \n",
    "i.e. that it in some sense understand what makes a tweet belong to that author.\n",
    "\n",
    "When we train the algorithm, it will only see the training data. \n",
    "Then, when we have finished training, we see how well it is doing on the test data.\n",
    "\n",
    "We split the data in 80% for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "train_ind = np.random.rand(X.shape[0]) < 0.7\n",
    "X_train = X[train_ind,]\n",
    "X_test = X[~train_ind,]\n",
    "y_train = y[train_ind]\n",
    "y_test = y[~train_ind]\n",
    "assert(len(np.unique(y_train)) ==  len(np.unique(y_test)))\n",
    "\n",
    "print(\"We have %d tweets in training set and %d tweets in the test set\"% (X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build machine learning model (start here!)\n",
    "This is where the machine learning starts.\n",
    "Hopefully, you havent really looked at what we did above, so there will be short recap of what we have right now.\n",
    "\n",
    "### Data:  \n",
    "We have also split our data into the training data (X_train, y_train), and the test data (X_test, y_test).\n",
    "When we build our model, we will only train it using the training data, but we will check how well it is doing it using the test data.\n",
    "\n",
    "- The tweets are stored in X_train and X_test.  \n",
    "- The authors of the corresponding tweets are stored in y_train and y_test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The first tweet in our training data looks like this: \")\n",
    "print(X_train[0,])\n",
    "print(\"We can decode it back to natural language by using the decoding function: \")\n",
    "print('--')\n",
    "print(decode_tweet(X_train[0,]))\n",
    "print('--')\n",
    "print(\"Then we can get the author of the tweet by looking in the y_train variable: \")\n",
    "print(\"--\")\n",
    "print(y[0])\n",
    "print(\"--\")\n",
    "print(\"Which corresponds to the author...: \")\n",
    "print(\"--\")\n",
    "print(ind2class[y[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "We will build our model in a framework called [Keras](http://keras.io).\n",
    "It is a deep learning framework using tensorflow as its backend (not important).\n",
    "\n",
    "We have gone through the model in the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_input = Input((PAD_LENGTH,))\n",
    "emb = Embedding(input_dim = len(ind2word), output_dim= 5)\n",
    "\n",
    "# We can represent each word as a vector of length 5:\n",
    "word_vectors = emb(tweet_input)\n",
    "\n",
    "# These word vectors are generic, \n",
    "# The simplest way to handle them is just to use the average word vector:\n",
    "avg_word_vectors = Lambda(lambda x: K.mean(x, 1))(word_vectors)\n",
    "\n",
    "# Add a hidden layer to abstract the average word:\n",
    "hidden_layer = Dense(10, activation = \"relu\")(avg_word_vectors)\n",
    "\n",
    "# Compute another layer. The output of this one is a vector of size N_Classes: \n",
    "# Each number is a probability that the tweet belongs to that class:\n",
    "probs = Dense(N_classes, activation = \"softmax\")(hidden_layer)\n",
    "\n",
    "\n",
    "model = Model(inputs = tweet_input, outputs = probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now defined in the 'model' object.\n",
    "If we want to inspect how it looks we can either call \"model.summary\" to see the whole model, \n",
    "or we can look at the individual layers by simply printing the layer out here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Print model:')\n",
    "print('--')\n",
    "print(model.summary())\n",
    "print(\"show how a specific layer looks like (most importantly the dimensions):\")\n",
    "print('--')\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "We have defined the model, but the model has not seen any datapoints yet.\n",
    "Do to so we need to feed it data.\n",
    "We have to add an optimizer (how the parameters of the model should be updated.\n",
    "Then we need to compile it and specify a couple of things:\n",
    "- The loss we want to minimize (basically \"sparse_categorical_crossentropy\" will try to increase probability of the right class towards 100% and decrease everything else to 0%).\n",
    "- We also specify that we want to monitor the accuracy (the share of observations that we correctly guess the right author).\n",
    "\n",
    "Calling model.fit starts a training procedure that will take some time.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr = 0.05)\n",
    "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "              optimizer = optimizer, \n",
    "              metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train, \n",
    "          verbose = 2,\n",
    "          validation_data = (X_test, y_test),\n",
    "         epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See how well the model works\n",
    "Now we have trained the model and we can monitor the val_acc metric above to see how often our model is guessing right.\n",
    "However, it does not say anything about what the model is doing right and what the model is doing wrong.\n",
    "For that, we can calculate a [confusion matrix](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/).\n",
    "A confusion matrix tells us where we predicted all tweets from a specific user.\n",
    "All rows is the real author, while the rows are the author the model predicted the tweets to be in.\n",
    "We have made the confusion matrix relative in percent.\n",
    "\n",
    "For example, if the 'audunstrand' row has 0.3 on the 'lenealexandra' column, it means that the model thinks 30% of audunstrand's tweets where from lenealexandra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relative_confusion_matrix(y_test,yhat):\n",
    "    confusion = confusion_matrix(y_test,yhat)\n",
    "\n",
    "    rel_confusion = np.round(confusion / confusion.sum(axis=1, keepdims = True),2)\n",
    "    rel_confusion = pd.DataFrame(rel_confusion, columns = ind2class.values(), index= ind2class.values())\n",
    "    rel_confusion.index.name = \"true\"\n",
    "    rel_confusion.columns.name = \"predicted\"\n",
    "    return rel_confusion\n",
    "\n",
    "# Predict probabilities for each class:\n",
    "yhat_probs = model.predict(X_test)\n",
    "\n",
    "# For each observation we take the author with highest probability as the predicted author:\n",
    "yhat = yhat_probs.argmax(axis=1)\n",
    "\n",
    "rel_confusion = create_relative_confusion_matrix(y_test, yhat)\n",
    "rel_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "sns.heatmap(rel_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also show some random errors so that we get the feel of what the model misses to see:\n",
    "\n",
    "def show_random_error():\n",
    "    errors = np.where(y_test != yhat)[0]\n",
    "    idx = np.random.choice(errors)\n",
    "    print('--- Tweet: ---')\n",
    "    print(decode_tweet(X_test[idx,]))\n",
    "    \n",
    "    print(\"Model believes:\\t %s\" %ind2class[yhat[idx]])\n",
    "    print(\"True author:\\t %s\" %ind2class[y_test[idx]])\n",
    "    \n",
    "show_random_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it's your turn..\n",
    "\n",
    "Are you able to make a better model than the one we just ran? \n",
    "\n",
    "Possible extensions of the model:\n",
    "\n",
    "- Try to increase the width of the dense layer (https://keras.io/layers/core/#dense)\n",
    "- Try to adjust the learning rate of your optimizer. Lower learning rates takes long but usually gives better results (https://keras.io/optimizers/)\n",
    "- Try to use a different optimizer. The Adam optizer is often a good alternative \n",
    "- Try to stack another dense layer on top of the one you have (or many!)\n",
    "- We are just using the \"average\" vector to predict. Could we try to stack the words horizontally instead? (https://keras.io/layers/core/#reshape)\n",
    "- Are the training accuracy much higher than the test accuracy? Try to add Dropout (https://keras.io/layers/core/#dropout) or Batch Normalization (https://keras.io/layers/normalization/)\n",
    "- Instead those average vectors, a very fancy alternative approach would be to try to model a tweet as a recurrent neural network (https://keras.io/layers/recurrent/)\n",
    "- There are millions of possible tricks. Try googling 'text classification twitter' or just 'text classification'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
